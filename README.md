# DPPO for Real-Time Adaptive PID Tuning
# DPPO å¯¦æ™‚è‡ªé©æ‡‰ PID èª¿åƒç³»çµ±

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Research](https://img.shields.io/badge/research-DPPO-red.svg)](RESEARCH_PLAN.md)

---

## Project Goal / é …ç›®ç›®æ¨™

**English:**
To implement a Deep Reinforcement Learning (DRL) agent using **Diffusion Policy Policy Optimization (DPPO)** as a Meta-Controller to learn optimal strategies for real-time adjustment of PID gains (K_p, K_i, K_d) for multi-axis control systems, specifically targeting inner-loop attitude rate control of a simulated quadrotor.

The project achieves **adaptive control** that is:
- **Robust** to disturbances and parameter variations
- **High-performance** in tracking diverse trajectories
- **Adaptive** compared to fixed controllers

**ä¸­æ–‡ï¼š**
æœ¬é …ç›®æ—¨åœ¨å¯¦ç¾ä¸€å€‹åŸºæ–¼**æ“´æ•£ç­–ç•¥ç­–ç•¥å„ªåŒ–ï¼ˆDPPOï¼‰**çš„æ·±åº¦å¼·åŒ–å­¸ç¿’ï¼ˆDRLï¼‰æ™ºèƒ½é«”ï¼Œä½œç‚ºå…ƒæ§åˆ¶å™¨å­¸ç¿’æœ€å„ªç­–ç•¥ï¼Œå¯¦æ™‚èª¿æ•´å¤šè»¸æ§åˆ¶ç³»çµ±çš„ PID å¢ç›Šï¼ˆK_p, K_i, K_dï¼‰ï¼Œç‰¹åˆ¥é‡å°æ¨¡æ“¬å››æ—‹ç¿¼é£›è¡Œå™¨çš„å…§ç’°å§¿æ…‹è§’é€Ÿç‡æ§åˆ¶ã€‚

é …ç›®å¯¦ç¾çš„**è‡ªé©æ‡‰æ§åˆ¶**å…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š
- å°æ“¾å‹•å’Œåƒæ•¸è®ŠåŒ–å…·æœ‰**é­¯æ£’æ€§**
- åœ¨è·Ÿè¹¤å¤šæ¨£åŒ–è»Œè·¡æ™‚å…·æœ‰**é«˜æ€§èƒ½**
- ç›¸æ¯”å›ºå®šæ§åˆ¶å™¨æ›´å…·**è‡ªé©æ‡‰æ€§**

---

## Research Focus / ç ”ç©¶é‡é»

**English:**
**Primary Focus**: Phase 3 - DPPO Policy Model Implementation (see [RESEARCH_PLAN.md](RESEARCH_PLAN.md))

This project represents a novel combination of:
- Diffusion Models for action generation
- PPO objectives for policy optimization
- Real-time PID parameter tuning

**ä¸­æ–‡ï¼š**
**ä¸»è¦ç ”ç©¶é‡é»**ï¼šç¬¬ä¸‰éšæ®µ - DPPO ç­–ç•¥æ¨¡å‹å¯¦ç¾ï¼ˆè©³è¦‹ [RESEARCH_PLAN.md](RESEARCH_PLAN.md)ï¼‰

æœ¬é …ç›®ä»£è¡¨äº†ä»¥ä¸‹æŠ€è¡“çš„å‰µæ–°çµåˆï¼š
- ç”¨æ–¼å‹•ä½œç”Ÿæˆçš„æ“´æ•£æ¨¡å‹
- ç”¨æ–¼ç­–ç•¥å„ªåŒ–çš„ PPO ç›®æ¨™
- å¯¦æ™‚ PID åƒæ•¸èª¿æ•´

---

## Development Phases / é–‹ç™¼éšæ®µ

```
Phase 1: Single-Axis Foundation âœ“ IMPLEMENTED
ç¬¬ä¸€éšæ®µï¼šå–®è»¸åŸºç¤ âœ“ å·²å¯¦ç¾
   â”œâ”€ 2nd-order system dynamics / äºŒéšç³»çµ±å‹•åŠ›å­¸
   â”œâ”€ RK4 integration / RK4 ç©åˆ†
   â”œâ”€ PID inner loop (200 Hz) / PID å…§ç’°ï¼ˆ200 Hzï¼‰
   â””â”€ PPO meta-controller (20 Hz) / PPO å…ƒæ§åˆ¶å™¨ï¼ˆ20 Hzï¼‰

Phase 2: DPPO MDP Definition âœ“ COMPLETE
ç¬¬äºŒéšæ®µï¼šDPPO MDP å®šç¾© âœ“ å®Œæˆ
   â”œâ”€ 9D observation space / 9 ç¶­è§€æ¸¬ç©ºé–“
   â”œâ”€ 3D action space / 3 ç¶­å‹•ä½œç©ºé–“
   â””â”€ Multi-objective reward function / å¤šç›®æ¨™çå‹µå‡½æ•¸

Phase 3: DPPO Implementation ğŸš§ IN PROGRESS (CORE RESEARCH)
ç¬¬ä¸‰éšæ®µï¼šDPPO å¯¦ç¾ ğŸš§ é€²è¡Œä¸­ï¼ˆæ ¸å¿ƒç ”ç©¶ï¼‰
   â”œâ”€ Diffusion model policy / æ“´æ•£æ¨¡å‹ç­–ç•¥
   â”œâ”€ PPO-weighted training / PPO åŠ æ¬Šè¨“ç·´
   â””â”€ Fast inference (<50ms) / å¿«é€Ÿæ¨ç†ï¼ˆ<50msï¼‰

Phase 4: 6-DOF Quadrotor ğŸ“‹ PLANNED
ç¬¬å››éšæ®µï¼š6-DOF å››æ—‹ç¿¼ ğŸ“‹ è¦åŠƒä¸­
   â”œâ”€ Full nonlinear dynamics / å®Œæ•´éç·šæ€§å‹•åŠ›å­¸
   â”œâ”€ Cascaded control (position â†’ attitude â†’ rate)
   â”‚  ç´šè¯æ§åˆ¶ï¼ˆä½ç½® â†’ å§¿æ…‹ â†’ è§’é€Ÿç‡ï¼‰
   â”œâ”€ 27+ dimensional state space / 27+ ç¶­ç‹€æ…‹ç©ºé–“
   â””â”€ 9D action space (3 axes Ã— 3 gains)
      9 ç¶­å‹•ä½œç©ºé–“ï¼ˆ3 è»¸ Ã— 3 å¢ç›Šï¼‰

Phase 5: Evaluation & Deployment ğŸ“‹ PLANNED
ç¬¬äº”éšæ®µï¼šè©•ä¼°èˆ‡éƒ¨ç½² ğŸ“‹ è¦åŠƒä¸­
   â”œâ”€ Baseline comparisons (Manual PID, LQR, Fixed RL-PID)
   â”‚  åŸºæº–å°æ¯”ï¼ˆæ‰‹å‹• PIDã€LQRã€å›ºå®š RL-PIDï¼‰
   â”œâ”€ Performance metrics (RMSE, settling time, robustness)
   â”‚  æ€§èƒ½æŒ‡æ¨™ï¼ˆRMSEã€ç©©å®šæ™‚é–“ã€é­¯æ£’æ€§ï¼‰
   â””â”€ Real-time deployment considerations
      å¯¦æ™‚éƒ¨ç½²è€ƒæ…®
```

ğŸ“– **See [RESEARCH_PLAN.md](RESEARCH_PLAN.md) for complete specifications**
ğŸ“– **è©³è¦‹ [RESEARCH_PLAN.md](RESEARCH_PLAN.md) ç²å–å®Œæ•´è¦æ ¼èªªæ˜**

---

## Quick Start (Phase 1 - Current) / å¿«é€Ÿé–‹å§‹ï¼ˆç¬¬ä¸€éšæ®µ - ç•¶å‰ï¼‰

```bash
# Install dependencies / å®‰è£ä¾è³´
pip install -r requirements.txt

# Run demo to test environment / é‹è¡Œæ¼”ç¤ºæ¸¬è©¦ç’°å¢ƒ
python demo.py

# Train PPO agent (Phase 1 baseline before DPPO)
# è¨“ç·´ PPO æ™ºèƒ½é«”ï¼ˆç¬¬ä¸€éšæ®µåŸºæº–ï¼ŒDPPO ä¹‹å‰ï¼‰
python train.py

# Evaluate trained model / è©•ä¼°è¨“ç·´æ¨¡å‹
python evaluate.py --model models/dppo_pid_final_*.zip

# Test DPPO model structure (Phase 3)
# æ¸¬è©¦ DPPO æ¨¡å‹çµæ§‹ï¼ˆç¬¬ä¸‰éšæ®µï¼‰
python dppo_model.py
```

---

## Table of Contents / ç›®éŒ„

- [Project Structure / é …ç›®çµæ§‹](#project-structure--é …ç›®çµæ§‹)
- [Technology Stack / æŠ€è¡“æ£§](#technology-stack--æŠ€è¡“æ£§)
- [Installation / å®‰è£](#installation--å®‰è£)
- [System Architecture / ç³»çµ±æ¶æ§‹](#system-architecture--ç³»çµ±æ¶æ§‹)
- [Usage / ä½¿ç”¨æ–¹æ³•](#usage--ä½¿ç”¨æ–¹æ³•)
- [Configuration / é…ç½®](#configuration--é…ç½®)
- [Implementation Details / å¯¦ç¾ç´°ç¯€](#implementation-details--å¯¦ç¾ç´°ç¯€)
- [Results and Visualization / çµæœèˆ‡å¯è¦–åŒ–](#results-and-visualization--çµæœèˆ‡å¯è¦–åŒ–)
- [Advanced Topics / é€²éšä¸»é¡Œ](#advanced-topics--é€²éšä¸»é¡Œ)

---

## Project Structure / é …ç›®çµæ§‹

```
DPPO_PID_controller/
â”œâ”€â”€ Phase 1 & 2: Single-Axis with PPO
â”‚   ç¬¬ä¸€ã€äºŒéšæ®µï¼šåŸºæ–¼ PPO çš„å–®è»¸ç³»çµ±
â”‚   â”œâ”€â”€ dppo_pid_env.py          # Gymnasium environment (Phase 1)
â”‚   â”‚                            # Gymnasium ç’°å¢ƒï¼ˆç¬¬ä¸€éšæ®µï¼‰
â”‚   â”œâ”€â”€ train.py                 # PPO training script / PPO è¨“ç·´è…³æœ¬
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation and visualization
â”‚   â”‚                            # è©•ä¼°èˆ‡å¯è¦–åŒ–
â”‚   â”œâ”€â”€ demo.py                  # Demo/testing script / æ¼”ç¤º/æ¸¬è©¦è…³æœ¬
â”‚   â””â”€â”€ config.yaml              # Phase 1 configuration / ç¬¬ä¸€éšæ®µé…ç½®
â”‚
â”œâ”€â”€ Phase 3: DPPO Implementation (CORE)
â”‚   ç¬¬ä¸‰éšæ®µï¼šDPPO å¯¦ç¾ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ dppo_model.py            # DPPO model (ğŸš§ skeleton)
â”‚   â”‚                            # DPPO æ¨¡å‹ï¼ˆğŸš§ éª¨æ¶ï¼‰
â”‚   â””â”€â”€ train_dppo.py            # DPPO training (TODO)
â”‚                                # DPPO è¨“ç·´ï¼ˆå¾…å¯¦ç¾ï¼‰
â”‚
â”œâ”€â”€ Phase 4: 6-DOF Quadrotor
â”‚   ç¬¬å››éšæ®µï¼š6-DOF å››æ—‹ç¿¼
â”‚   â”œâ”€â”€ quadrotor_6dof_env.py    # 6-DOF environment (ğŸ“‹ placeholder)
â”‚   â”‚                            # 6-DOF ç’°å¢ƒï¼ˆğŸ“‹ ä½”ä½ç¬¦ï¼‰
â”‚   â””â”€â”€ config_6dof.yaml         # Phase 4 configuration (TODO)
â”‚                                # ç¬¬å››éšæ®µé…ç½®ï¼ˆå¾…å¯¦ç¾ï¼‰
â”‚
â”œâ”€â”€ Documentation / æ–‡æª”
â”‚   â”œâ”€â”€ README.md                # This file / æœ¬æ–‡ä»¶
â”‚   â”œâ”€â”€ RESEARCH_PLAN.md         # Complete research plan / å®Œæ•´ç ”ç©¶è¨ˆåŠƒ
â”‚   â””â”€â”€ PPO_HYPERPARAMETERS.md   # Hyperparameter guide (ä¸­è‹±æ–‡)
â”‚                                # è¶…åƒæ•¸æŒ‡å—ï¼ˆä¸­è‹±æ–‡ï¼‰
â”‚
â””â”€â”€ Configuration / é…ç½®
    â”œâ”€â”€ requirements.txt         # Python dependencies / Python ä¾è³´
    â””â”€â”€ .gitignore               # Git ignore patterns / Git å¿½ç•¥æ¨¡å¼
```

---

## Technology Stack / æŠ€è¡“æ£§

| Component<br>çµ„ä»¶ | Technology<br>æŠ€è¡“ | Role<br>ä½œç”¨ |
|-----------|-----------|------|
| RL Framework<br>å¼·åŒ–å­¸ç¿’æ¡†æ¶ | Stable-Baselines3 (PPO) | Implements the DRL algorithm and training loop<br>å¯¦ç¾ DRL ç®—æ³•å’Œè¨“ç·´å¾ªç’° |
| Diffusion Model<br>æ“´æ•£æ¨¡å‹ | PyTorch (custom)<br>PyTorchï¼ˆè‡ªå®šç¾©ï¼‰ | DPPO policy network for action generation<br>ç”¨æ–¼å‹•ä½œç”Ÿæˆçš„ DPPO ç­–ç•¥ç¶²çµ¡ |
| Simulation Environment<br>æ¨¡æ“¬ç’°å¢ƒ | Farama Gymnasium (custom)<br>Farama Gymnasiumï¼ˆè‡ªå®šç¾©ï¼‰ | Defines state, action, reward, and transition dynamics<br>å®šç¾©ç‹€æ…‹ã€å‹•ä½œã€çå‹µå’Œè½‰ç§»å‹•åŠ›å­¸ |
| System Dynamics (Plant)<br>ç³»çµ±å‹•åŠ›å­¸ï¼ˆè¢«æ§å°è±¡ï¼‰ | NumPy | High-speed numerical integration for physics model<br>ç‰©ç†æ¨¡å‹çš„é«˜é€Ÿæ•¸å€¼ç©åˆ† |
| Logging/Visualization<br>æ—¥èªŒ/å¯è¦–åŒ– | TensorBoard & Matplotlib | Tracks learning progress and performance metrics<br>è¿½è¹¤å­¸ç¿’é€²åº¦å’Œæ€§èƒ½æŒ‡æ¨™ |

---

## Installation / å®‰è£

### Prerequisites / å‰ç½®è¦æ±‚

**English:**
- Python 3.8 or higher
- pip package manager
- (Optional) CUDA-capable GPU for Phase 3 DPPO training

**ä¸­æ–‡ï¼š**
- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- pip å¥—ä»¶ç®¡ç†å™¨
- ï¼ˆå¯é¸ï¼‰æ”¯æŒ CUDA çš„ GPU ç”¨æ–¼ç¬¬ä¸‰éšæ®µ DPPO è¨“ç·´

### Setup / è¨­ç½®

```bash
# Clone the repository / å…‹éš†å€‰åº«
git clone <repository-url>
cd DPPO_PID_controller

# Create a virtual environment
python -m venv dppo  

# Activate the virtual environment
.\dppo\Scripts\activate 

# Install dependencies / å®‰è£ä¾è³´
pip install -r requirements.txt
```

---

## System Architecture / ç³»çµ±æ¶æ§‹

**English:**
The system implements a **dual-loop control architecture**:

**ä¸­æ–‡ï¼š**
ç³»çµ±å¯¦ç¾äº†**é›™è¿´è·¯æ§åˆ¶æ¶æ§‹**ï¼š

### Two-Timescale Control Loop / é›™æ™‚é–“å°ºåº¦æ§åˆ¶è¿´è·¯

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Meta-Controller / å…ƒæ§åˆ¶å™¨               â”‚
â”‚        (PPO/DPPO RL Agent @ 20 Hz)                   â”‚
â”‚        (PPO/DPPO å¼·åŒ–å­¸ç¿’æ™ºèƒ½é«” @ 20 Hz)             â”‚
â”‚                                                       â”‚
â”‚  Inputs / è¼¸å…¥: [error, error_dot, integral, x,      â”‚
â”‚                  x_dot, reference, Kp, Ki, Kd]       â”‚
â”‚  Outputs / è¼¸å‡º: [Kp_new, Ki_new, Kd_new]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Updates every 0.05s / æ¯ 0.05 ç§’æ›´æ–°
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Inner PID Controller / å…§ç’° PID æ§åˆ¶å™¨       â”‚
â”‚                  (@ 200 Hz)                          â”‚
â”‚                                                       â”‚
â”‚  u(t) = KpÂ·e(t) + KiÂ·âˆ«e(t)dt + KdÂ·de(t)/dt         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Control signal u(t) / æ§åˆ¶ä¿¡è™Ÿ u(t)
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       2nd-Order Plant System / äºŒéšè¢«æ§ç³»çµ±          â”‚
â”‚                                                       â”‚
â”‚          JÂ·áº + BÂ·áº‹ = u(t) + d(t)                    â”‚
â”‚                                                       â”‚
â”‚  J: Inertia (1.0) / æ…£é‡ (1.0)                      â”‚
â”‚  B: Damping (0.5) / é˜»å°¼ (0.5)                      â”‚
â”‚  d: External disturbance / å¤–éƒ¨æ“¾å‹•                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Timing Configuration / æ™‚åºé…ç½®

| Parameter<br>åƒæ•¸ | Value<br>æ•¸å€¼ | Description<br>æè¿° |
|-----------|-------|-------------|
| Inner Loop Î”t<br>å…§ç’° Î”t | 0.005s (200 Hz) | PID calculation and physics integration<br>PID è¨ˆç®—å’Œç‰©ç†ç©åˆ† |
| Outer Loop Î”t<br>å¤–ç’° Î”t | 0.05s (20 Hz) | RL agent updates PID gains<br>RL æ™ºèƒ½é«”æ›´æ–° PID å¢ç›Š |
| Steps per RL Action<br>æ¯å€‹ RL å‹•ä½œçš„æ­¥æ•¸ | 10 | Inner loop steps per outer loop step<br>æ¯å€‹å¤–ç’°æ­¥é©Ÿçš„å…§ç’°æ­¥æ•¸ |

### Plant Dynamics / è¢«æ§å°è±¡å‹•åŠ›å­¸

**English:**
The environment simulates a **2nd-order linear system**:

**ä¸­æ–‡ï¼š**
ç’°å¢ƒæ¨¡æ“¬ä¸€å€‹**äºŒéšç·šæ€§ç³»çµ±**ï¼š

```
JÂ·áº + BÂ·áº‹ = u(t) + d(t)
```

**Where / å…¶ä¸­ï¼š**
- **x**: Position/angle of the system / ç³»çµ±çš„ä½ç½®/è§’åº¦
- **u(t)**: Control output from PID controller / PID æ§åˆ¶å™¨çš„æ§åˆ¶è¼¸å‡º
- **d(t)**: External disturbance (random, time-limited) / å¤–éƒ¨æ“¾å‹•ï¼ˆéš¨æ©Ÿã€æ™‚é™ï¼‰
- **J**: Inertia coefficient (default: 1.0) / æ…£é‡ä¿‚æ•¸ï¼ˆé»˜èªï¼š1.0ï¼‰
- **B**: Damping coefficient (default: 0.5) / é˜»å°¼ä¿‚æ•¸ï¼ˆé»˜èªï¼š0.5ï¼‰

### PID Controller / PID æ§åˆ¶å™¨

**Standard parallel-form PID / æ¨™æº–ä¸¦è¯å¼ PIDï¼š**

```
u(t) = KpÂ·e(t) + KiÂ·âˆ«e(t)dt + KdÂ·(e(t) - e(t-1))/Î”t
```

**Where / å…¶ä¸­ï¼š**
- **e(t) = r(t) - x(t)**: Tracking error / è·Ÿè¹¤èª¤å·®
- **Kp, Ki, Kd**: Gains adjusted by the RL agent / ç”± RL æ™ºèƒ½é«”èª¿æ•´çš„å¢ç›Š
- **r(t)**: Reference setpoint (changes periodically) / åƒè€ƒè¨­å®šå€¼ï¼ˆé€±æœŸæ€§è®ŠåŒ–ï¼‰

---

## Usage / ä½¿ç”¨æ–¹æ³•

### 1. Test the Environment (Demo) / æ¸¬è©¦ç’°å¢ƒï¼ˆæ¼”ç¤ºï¼‰

**English:**
Run the demo script to verify the environment works correctly:

**ä¸­æ–‡ï¼š**
é‹è¡Œæ¼”ç¤ºè…³æœ¬ä»¥é©—è­‰ç’°å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
python demo.py
```

**This will / é€™å°‡ï¼š**
- Test the environment API / æ¸¬è©¦ç’°å¢ƒ API
- Run an episode with random PID gains / ä½¿ç”¨éš¨æ©Ÿ PID å¢ç›Šé‹è¡Œä¸€å€‹å›åˆ
- Generate a visualization (`demo_results.png`) / ç”Ÿæˆå¯è¦–åŒ–çµæœï¼ˆ`demo_results.png`ï¼‰

### 2. Train the Agent / è¨“ç·´æ™ºèƒ½é«”

**English:**
Start training with default configuration:

**ä¸­æ–‡ï¼š**
ä½¿ç”¨é»˜èªé…ç½®é–‹å§‹è¨“ç·´ï¼š

```bash
python train.py
```

**With custom configuration / ä½¿ç”¨è‡ªå®šç¾©é…ç½®ï¼š**

```bash
python train.py --config my_config.yaml
```

**Resume training from a checkpoint / å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼š**

```bash
python train.py --resume --model models/dppo_pid_checkpoint_1000000_steps.zip
```

**Monitor training progress with TensorBoard / ä½¿ç”¨ TensorBoard ç›£æ§è¨“ç·´é€²åº¦ï¼š**

```bash
tensorboard --logdir ./ppo_pid_logs/
```

### 3. Evaluate Trained Model / è©•ä¼°è¨“ç·´æ¨¡å‹

**English:**
Evaluate and visualize performance:

**ä¸­æ–‡ï¼š**
è©•ä¼°ä¸¦å¯è¦–åŒ–æ€§èƒ½ï¼š

```bash
python evaluate.py --model models/dppo_pid_final_TIMESTAMP.zip --episodes 10
```

**This generates / é€™å°‡ç”Ÿæˆï¼š**
- Performance plots for best/worst episodes / æœ€ä½³/æœ€å·®å›åˆçš„æ€§èƒ½åœ–è¡¨
- Summary statistics across all episodes / æ‰€æœ‰å›åˆçš„çµ±è¨ˆæ‘˜è¦
- Saved in `./evaluation_results/` / ä¿å­˜åœ¨ `./evaluation_results/`

---

## Configuration / é…ç½®

**English:**
All hyperparameters are defined in `config.yaml`. Key sections:

**ä¸­æ–‡ï¼š**
æ‰€æœ‰è¶…åƒæ•¸åœ¨ `config.yaml` ä¸­å®šç¾©ã€‚ä¸»è¦éƒ¨åˆ†ï¼š

### Plant Parameters / è¢«æ§å°è±¡åƒæ•¸

```yaml
plant:
  J: 1.0      # Inertia / æ…£é‡
  B: 0.5      # Damping / é˜»å°¼
  u_min: -10.0  # Min control / æœ€å°æ§åˆ¶
  u_max: 10.0   # Max control / æœ€å¤§æ§åˆ¶
  integration_method: "rk4"  # "euler" or "rk4"
```

### Reward Weights / çå‹µæ¬Šé‡

```yaml
reward:
  lambda_error: 5.0       # Tracking error weight / è·Ÿè¹¤èª¤å·®æ¬Šé‡
  lambda_velocity: 0.5    # Velocity penalty weight / é€Ÿåº¦æ‡²ç½°æ¬Šé‡
  lambda_control: 0.01    # Control effort weight / æ§åˆ¶åŠªåŠ›æ¬Šé‡
  lambda_overshoot: 0.2   # Overshoot penalty weight / è¶…èª¿æ‡²ç½°æ¬Šé‡
```

### PPO Training Parameters / PPO è¨“ç·´åƒæ•¸

```yaml
training:
  total_timesteps: 5000000  # Total training steps / ç¸½è¨“ç·´æ­¥æ•¸
  learning_rate: 0.0003     # Learning rate / å­¸ç¿’ç‡
  n_steps: 2048             # Trajectory length / è»Œè·¡é•·åº¦
  batch_size: 64            # Mini-batch size / å°æ‰¹é‡å¤§å°
  gamma: 0.99               # Discount factor / æŠ˜æ‰£å› å­
```

**See `config.yaml` for all available parameters.**
**æŸ¥çœ‹ `config.yaml` ç²å–æ‰€æœ‰å¯ç”¨åƒæ•¸ã€‚**

---

## Implementation Details / å¯¦ç¾ç´°ç¯€

### Markov Decision Process (MDP) / é¦¬çˆ¾å¯å¤«æ±ºç­–éç¨‹ (MDP)

#### Action Space (ğ€) / å‹•ä½œç©ºé–“ (ğ€)

**Dimensions / ç¶­åº¦**: 3 (continuous / é€£çºŒ)
**Range / ç¯„åœ**: [0.0, K_max]
- Kp_max = 10.0
- Ki_max = 5.0
- Kd_max = 5.0

**Components / çµ„æˆ**: [K_p, K_i, K_d]

**English:**
The agent directly outputs new PID gain values, bounded to prevent instability.

**ä¸­æ–‡ï¼š**
æ™ºèƒ½é«”ç›´æ¥è¼¸å‡ºæ–°çš„ PID å¢ç›Šå€¼ï¼Œæœ‰ç•Œä»¥é˜²æ­¢ä¸ç©©å®šã€‚

#### Observation Space (ğ’) / è§€æ¸¬ç©ºé–“ (ğ’)

**Dimensions / ç¶­åº¦**: 9 (continuous, normalized to [-1, 1] / é€£çºŒï¼Œæ­¸ä¸€åŒ–åˆ° [-1, 1])

**Components / çµ„æˆï¼š**

1. Current position error (e) / ç•¶å‰ä½ç½®èª¤å·® (e)
2. Error derivative (Ä—) / èª¤å·®å°æ•¸ (Ä—)
3. Accumulated error (integral term) / ç´¯ç©èª¤å·®ï¼ˆç©åˆ†é …ï¼‰
4. System position (x) / ç³»çµ±ä½ç½® (x)
5. System velocity (áº‹) / ç³»çµ±é€Ÿåº¦ (áº‹)
6. Target reference (r) / ç›®æ¨™åƒè€ƒå€¼ (r)
7. Current Kp / ç•¶å‰ Kp
8. Current Ki / ç•¶å‰ Ki
9. Current Kd / ç•¶å‰ Kd

**English:**
Including current gains enables the agent to learn **relative adjustments**.

**ä¸­æ–‡ï¼š**
åŒ…å«ç•¶å‰å¢ç›Šä½¿æ™ºèƒ½é«”èƒ½å¤ å­¸ç¿’**ç›¸å°èª¿æ•´**ã€‚

#### Reward Function (ğ‘) / çå‹µå‡½æ•¸ (ğ‘)

**English:**
Multi-objective reward combining four components:

**ä¸­æ–‡ï¼š**
çµåˆå››å€‹çµ„æˆéƒ¨åˆ†çš„å¤šç›®æ¨™çå‹µï¼š

```python
R = -Î»â‚Â·eÂ² - Î»â‚‚Â·áº‹Â² - Î»â‚ƒÂ·uÂ² - Î»â‚„Â·max(0, eÂ·Ä—)
```

| Component<br>çµ„æˆéƒ¨åˆ† | Weight (Î»)<br>æ¬Šé‡ (Î») | Purpose<br>ç›®çš„ |
|-----------|------------|---------|
| Tracking Error<br>è·Ÿè¹¤èª¤å·® | Î»â‚ = 5.0 | Minimize deviation from setpoint<br>æœ€å°åŒ–èˆ‡è¨­å®šå€¼çš„åå·® |
| Velocity Penalty<br>é€Ÿåº¦æ‡²ç½° | Î»â‚‚ = 0.5 | Reduce oscillations<br>æ¸›å°‘æŒ¯ç›ª |
| Control Effort<br>æ§åˆ¶åŠªåŠ› | Î»â‚ƒ = 0.01 | Energy efficiency<br>èƒ½é‡æ•ˆç‡ |
| Overshoot<br>è¶…èª¿ | Î»â‚„ = 0.2 | Penalize moving away from setpoint<br>æ‡²ç½°é é›¢è¨­å®šå€¼çš„ç§»å‹• |

#### Episode Termination / å›åˆçµ‚æ­¢

**English:**
An episode ends when:

**ä¸­æ–‡ï¼š**
ç•¶ä»¥ä¸‹æƒ…æ³ç™¼ç”Ÿæ™‚å›åˆçµæŸï¼š

- Position exceeds safety bounds (|x| > 5.0) â†’ system instability
  ä½ç½®è¶…å‡ºå®‰å…¨ç•Œé™ (|x| > 5.0) â†’ ç³»çµ±ä¸ç©©å®š
- Maximum steps reached (1000 steps = 50 seconds)
  é”åˆ°æœ€å¤§æ­¥æ•¸ï¼ˆ1000 æ­¥ = 50 ç§’ï¼‰

### Reference Signal / åƒè€ƒä¿¡è™Ÿ

**English:**
To encourage adaptation, the setpoint changes periodically:

**ä¸­æ–‡ï¼š**
ç‚ºäº†é¼“å‹µè‡ªé©æ‡‰ï¼Œè¨­å®šå€¼é€±æœŸæ€§è®ŠåŒ–ï¼š

- Changes every 2 seconds / æ¯ 2 ç§’è®ŠåŒ–ä¸€æ¬¡
- Random value in [-2, 2] / [-2, 2] ç¯„åœå…§çš„éš¨æ©Ÿå€¼
- Forces agent to re-tune gains for different operating conditions
  å¼·åˆ¶æ™ºèƒ½é«”é‡å°ä¸åŒå·¥ä½œæ¢ä»¶é‡æ–°èª¿æ•´å¢ç›Š

### Disturbances / æ“¾å‹•

**English:**
Random external disturbances test robustness:

**ä¸­æ–‡ï¼š**
éš¨æ©Ÿå¤–éƒ¨æ“¾å‹•æ¸¬è©¦é­¯æ£’æ€§ï¼š

- Magnitude: Â±0.5 / å¹…åº¦ï¼šÂ±0.5
- Duration: 0.1 seconds / æŒçºŒæ™‚é–“ï¼š0.1 ç§’
- Occurs randomly with low probability / ä»¥ä½æ¦‚ç‡éš¨æ©Ÿç™¼ç”Ÿ

---

## Results and Visualization / çµæœèˆ‡å¯è¦–åŒ–

### Training Metrics (TensorBoard) / è¨“ç·´æŒ‡æ¨™ (TensorBoard)

**Monitor during training / è¨“ç·´æœŸé–“ç›£æ§ï¼š**
- Episode reward (cumulative) / å›åˆçå‹µï¼ˆç´¯ç©ï¼‰
- Episode length / å›åˆé•·åº¦
- Policy loss / Value loss / ç­–ç•¥æå¤± / åƒ¹å€¼æå¤±
- Entropy (exploration) / ç†µï¼ˆæ¢ç´¢ï¼‰

### Evaluation Plots / è©•ä¼°åœ–è¡¨

**English:**
The evaluation script generates:

**ä¸­æ–‡ï¼š**
è©•ä¼°è…³æœ¬ç”Ÿæˆï¼š

1. **Position Tracking** / **ä½ç½®è·Ÿè¹¤**: Shows system response vs. reference signal
   é¡¯ç¤ºç³»çµ±éŸ¿æ‡‰èˆ‡åƒè€ƒä¿¡è™Ÿ
2. **Tracking Error** / **è·Ÿè¹¤èª¤å·®**: Error over time / éš¨æ™‚é–“è®ŠåŒ–çš„èª¤å·®
3. **Control Input** / **æ§åˆ¶è¼¸å…¥**: PID output force/torque / PID è¼¸å‡ºåŠ›/åŠ›çŸ©
4. **PID Gains Evolution** / **PID å¢ç›Šæ¼”åŒ–**: How Kp, Ki, Kd change in real-time
   Kp, Ki, Kd å¦‚ä½•å¯¦æ™‚è®ŠåŒ–
5. **Summary Statistics** / **çµ±è¨ˆæ‘˜è¦**: Rewards, errors, and gain distributions
   çå‹µã€èª¤å·®å’Œå¢ç›Šåˆ†ä½ˆ

---

## Advanced Topics / é€²éšä¸»é¡Œ

### Curriculum Learning / èª²ç¨‹å­¸ç¿’

**English:**
For faster training, consider implementing curriculum learning:

**ä¸­æ–‡ï¼š**
ç‚ºäº†æ›´å¿«è¨“ç·´ï¼Œè€ƒæ…®å¯¦æ–½èª²ç¨‹å­¸ç¿’ï¼š

1. **Phase 1** / **éšæ®µ 1**: Simple step references, no disturbances
   ç°¡å–®éšèºåƒè€ƒï¼Œç„¡æ“¾å‹•
2. **Phase 2** / **éšæ®µ 2**: Introduce reference changes / å¼•å…¥åƒè€ƒè®ŠåŒ–
3. **Phase 3** / **éšæ®µ 3**: Add external disturbances / æ·»åŠ å¤–éƒ¨æ“¾å‹•
4. **Phase 4** / **éšæ®µ 4**: Increase disturbance magnitude / å¢åŠ æ“¾å‹•å¹…åº¦

**Modify `config.yaml` between phases or implement automatic curriculum in the environment.**
**åœ¨éšæ®µä¹‹é–“ä¿®æ”¹ `config.yaml` æˆ–åœ¨ç’°å¢ƒä¸­å¯¦ç¾è‡ªå‹•èª²ç¨‹ã€‚**

### Hyperparameter Tuning / è¶…åƒæ•¸èª¿æ•´

**English:**
Key parameters to tune:

**ä¸­æ–‡ï¼š**
éœ€è¦èª¿æ•´çš„é—œéµåƒæ•¸ï¼š

**Reward Weights / çå‹µæ¬Šé‡**: Balance between tracking, stability, and efficiency
å¹³è¡¡è·Ÿè¹¤ã€ç©©å®šæ€§å’Œæ•ˆç‡
- Increase `lambda_error` if tracking is poor / å¦‚æœè·Ÿè¹¤æ•ˆæœå·®å‰‡å¢åŠ  `lambda_error`
- Increase `lambda_overshoot` if oscillations occur / å¦‚æœç™¼ç”ŸæŒ¯ç›ªå‰‡å¢åŠ  `lambda_overshoot`
- Adjust `lambda_velocity` for smoother control / èª¿æ•´ `lambda_velocity` ä»¥ç²å¾—æ›´å¹³æ»‘çš„æ§åˆ¶

**PPO Parameters / PPO åƒæ•¸**:
- `learning_rate`: Lower (1e-4) for stability, higher (5e-4) for faster learning
  é™ä½ï¼ˆ1e-4ï¼‰ä»¥æé«˜ç©©å®šæ€§ï¼Œæé«˜ï¼ˆ5e-4ï¼‰ä»¥åŠ å¿«å­¸ç¿’
- `n_steps`: More steps = more data per update (but slower)
  æ›´å¤šæ­¥æ•¸ = æ¯æ¬¡æ›´æ–°æ›´å¤šæ•¸æ“šï¼ˆä½†æ›´æ…¢ï¼‰
- `batch_size`: Larger batches = more stable gradients
  æ›´å¤§çš„æ‰¹æ¬¡ = æ›´ç©©å®šçš„æ¢¯åº¦

### Extensions / æ“´å±•

**English:**
Potential improvements:

**ä¸­æ–‡ï¼š**
æ½›åœ¨æ”¹é€²ï¼š

1. **Multi-axis control** / **å¤šè»¸æ§åˆ¶**: Extend to 3D systems (quadrotors, robot arms)
   æ“´å±•åˆ° 3D ç³»çµ±ï¼ˆå››æ—‹ç¿¼ã€æ©Ÿå™¨äººæ‰‹è‡‚ï¼‰
2. **Model-based approaches** / **åŸºæ–¼æ¨¡å‹çš„æ–¹æ³•**: Incorporate system identification
   çµåˆç³»çµ±è¾¨è­˜
3. **Domain randomization** / **é ˜åŸŸéš¨æ©ŸåŒ–**: Vary plant parameters (J, B) during training
   è¨“ç·´æœŸé–“æ”¹è®Šè¢«æ§å°è±¡åƒæ•¸ï¼ˆJ, Bï¼‰
4. **Real-world transfer** / **å¯¦éš›æ‡‰ç”¨é·ç§»**: Deploy on hardware with sim-to-real techniques
   ä½¿ç”¨ä»¿çœŸåˆ°ç¾å¯¦æŠ€è¡“éƒ¨ç½²åˆ°ç¡¬ä»¶
5. **Hierarchical control** / **åˆ†å±¤æ§åˆ¶**: Add higher-level trajectory planning
   æ·»åŠ æ›´é«˜å±¤æ¬¡çš„è»Œè·¡è¦åŠƒ

### C++ Integration (Optional) / C++ é›†æˆï¼ˆå¯é¸ï¼‰

**English:**
For high-fidelity simulation or hardware deployment:

**ä¸­æ–‡ï¼š**
ç”¨æ–¼é«˜ä¿çœŸæ¨¡æ“¬æˆ–ç¡¬ä»¶éƒ¨ç½²ï¼š

1. **Plant in C++** / **C++ è¢«æ§å°è±¡**: Use Eigen for dynamics, expose via pybind11
   ä½¿ç”¨ Eigen é€²è¡Œå‹•åŠ›å­¸è¨ˆç®—ï¼Œé€šé pybind11 æš´éœ²
2. **Low-latency PID** / **ä½å»¶é² PID**: C++ inner loop for realistic timing
   C++ å…§ç’°å¯¦ç¾çœŸå¯¦æ™‚åº
3. **ROS/Gazebo**: Integrate with robotics middleware / èˆ‡æ©Ÿå™¨äººä¸­é–“ä»¶é›†æˆ
4. **Hardware-in-the-loop** / **ç¡¬ä»¶åœ¨ç’°**: Test on actual systems / åœ¨å¯¦éš›ç³»çµ±ä¸Šæ¸¬è©¦

---

## Evaluation Metrics / è©•ä¼°æŒ‡æ¨™

**English:**
Compare against baselines:

**ä¸­æ–‡ï¼š**
èˆ‡åŸºæº–é€²è¡Œæ¯”è¼ƒï¼š

| Metric<br>æŒ‡æ¨™ | Description<br>æè¿° | Better<br>æ›´å¥½ |
|--------|-------------|--------|
| ISE | Integrated Squared Error<br>ç©åˆ†å¹³æ–¹èª¤å·® | Lower â†“<br>è¶Šä½è¶Šå¥½ |
| RMSE | Root Mean Square Error<br>å‡æ–¹æ ¹èª¤å·® | Lower â†“<br>è¶Šä½è¶Šå¥½ |
| Settling Time<br>ç©©å®šæ™‚é–“ | Time to reach Â±2% of setpoint<br>é”åˆ°è¨­å®šå€¼ Â±2% çš„æ™‚é–“ | Lower â†“<br>è¶Šä½è¶Šå¥½ |
| Overshoot %<br>è¶…èª¿ç™¾åˆ†æ¯” | Maximum overshoot percentage<br>æœ€å¤§è¶…èª¿ç™¾åˆ†æ¯” | Lower â†“<br>è¶Šä½è¶Šå¥½ |
| Control Effort<br>æ§åˆ¶åŠªåŠ› | Sum of squared control inputs<br>æ§åˆ¶è¼¸å…¥å¹³æ–¹å’Œ | Lower â†“<br>è¶Šä½è¶Šå¥½ |

---

## Citation / å¼•ç”¨

**English:**
If you use this code in your research, please cite:

**ä¸­æ–‡ï¼š**
å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨æ­¤ä»£ç¢¼ï¼Œè«‹å¼•ç”¨ï¼š

```bibtex
@software{dppo_pid_controller,
  title = {DPPO for Real-Time Adaptive PID Tuning},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/DPPO_PID_controller}
}
```

---

## License / è¨±å¯è­‰

This project is licensed under the MIT License.
æœ¬é …ç›®æ ¹æ“š MIT è¨±å¯è­‰æˆæ¬Šã€‚

---

## Contributing / è²¢ç»

**English:**
Contributions are welcome! Please:

**ä¸­æ–‡ï¼š**
æ­¡è¿è²¢ç»ï¼è«‹ï¼š

1. Fork the repository / åˆ†å‰å€‰åº«
2. Create a feature branch / å‰µå»ºåŠŸèƒ½åˆ†æ”¯
3. Commit your changes / æäº¤æ‚¨çš„æ›´æ”¹
4. Push to the branch / æ¨é€åˆ°åˆ†æ”¯
5. Open a Pull Request / æ‰“é–‹æ‹‰å–è«‹æ±‚

---

## Acknowledgments / è‡´è¬

- **Stable-Baselines3**: Robust PPO implementation / å¼·å¤§çš„ PPO å¯¦ç¾
- **Gymnasium**: Clean RL environment interface / æ¸…æ™°çš„ RL ç’°å¢ƒæ¥å£
- **OpenAI**: Original PPO algorithm / åŸå§‹ PPO ç®—æ³•

---

## References / åƒè€ƒæ–‡ç»

1. Schulman et al. (2017). "Proximal Policy Optimization Algorithms"
2. Lillicrap et al. (2015). "Continuous control with deep reinforcement learning"
3. Ã…strÃ¶m & Murray (2008). "Feedback Systems: An Introduction for Scientists and Engineers"
4. Ho et al. (2020). "Denoising Diffusion Probabilistic Models"
5. Song et al. (2020). "Denoising Diffusion Implicit Models"

---

## Contact / è¯ç¹«æ–¹å¼

**English:**
For questions or issues, please open an issue on GitHub.

**ä¸­æ–‡ï¼š**
å¦‚æœ‰å•é¡Œæˆ–ç–‘å•ï¼Œè«‹åœ¨ GitHub ä¸Šé–‹å•Ÿ issueã€‚

---

**ğŸ“– For detailed research plan, see [RESEARCH_PLAN.md](RESEARCH_PLAN.md)**
**ğŸ“– è©³ç´°ç ”ç©¶è¨ˆåŠƒè«‹è¦‹ [RESEARCH_PLAN.md](RESEARCH_PLAN.md)**

**ğŸ“– For PPO hyperparameter tuning guide, see [PPO_HYPERPARAMETERS.md](PPO_HYPERPARAMETERS.md)**
**ğŸ“– PPO è¶…åƒæ•¸èª¿æ•´æŒ‡å—è«‹è¦‹ [PPO_HYPERPARAMETERS.md](PPO_HYPERPARAMETERS.md)**
