# DPPO PID Controller Configuration
# Based on Research Plan Phase 1: Single-Axis Environment Setup

# Plant Dynamics (Second-Order System - Angular Rate Control)
plant:
  J: 1.0                    # Inertia (moment of inertia)
  B: 0.5                    # Damping coefficient
  u_min: -10.0              # Minimum control input (actuator limit)
  u_max: 10.0               # Maximum control input (actuator limit)
  integration_method: "rk4" # Integration method: "euler" or "rk4" (RK4 recommended)

# Time Step Configuration
timing:
  dt_inner: 0.005           # Inner loop time step (200 Hz)
  dt_outer: 0.05            # Outer loop time step (20 Hz)
  n_inner_steps: 10         # Steps per RL action (dt_outer / dt_inner)

# Reference Signal
reference:
  r_min: -2.0               # Minimum setpoint
  r_max: 2.0                # Maximum setpoint
  change_interval: 2.0      # Seconds between setpoint changes

# Disturbance (Intermittent Pulse Disturbance)
disturbance:
  enabled: true
  magnitude: 0.5            # Max disturbance force: ±0.5
  duration: 0.1             # Duration of disturbance (seconds)
  interval: 5.0             # Average time between disturbances (seconds)
  probability: 0.01         # Probability of disturbance per inner step

# PID Constraints (Phase 1 Specifications)
pid:
  # Controller type (實驗性功能)
  controller_type: "linear"  # "linear" 或 "nonlinear" (AirPilot 風格)
  nonlinear_max_velocity: 1.0  # 非線性 PID 的最大速度限制

  # Initial values (manual baseline)
  kp_init: 5.0              # Initial proportional gain
  ki_init: 0.1              # Initial integral gain
  kd_init: 0.2              # Initial derivative gain

  # Maximum bounds (action clipping)
  kp_max: 10.0              # Maximum proportional gain
  ki_max: 5.0               # Maximum integral gain
  kd_max: 5.0               # Maximum derivative gain

  # Anti-windup
  integral_max: 100.0       # Anti-windup limit for integral term

# Observation Space Normalization
observation:
  error_scale: 2.0
  error_dot_scale: 5.0
  integral_scale: 10.0
  position_scale: 3.0
  velocity_scale: 5.0
  reference_scale: 2.0
  gain_scale: 10.0

# Reward Function Configuration
# 獎勵函數類型選擇：
#   - "continuous": 連續型懲罰獎勵（負數，適合連續跟蹤任務）- 舊版默認
#   - "gaussian": 高斯型獎勵（0-1，正數，適合強化學習控制任務）- 推薦
#   - "task_completion": 任務完成型獎勵（正數，AirPilot 風格，適合點對點導航）- 實驗性
reward:
  reward_type: "gaussian"  # 切換到高斯型獎勵

  # 高斯型獎勵參數（reward_type: "gaussian" 時使用）
  # r = w_err * exp(-error^2 / sigma_err) + w_vel * exp(-velocity^2 / sigma_vel) + stability_bonus
  gaussian:
    sigma_error: 0.5        # 誤差的高斯寬度（越小越嚴格）
    sigma_velocity: 1.0     # 速度的高斯寬度
    w_error: 0.7            # 誤差項權重
    w_velocity: 0.3         # 速度項權重
    w_action: 0.05          # 動作懲罰權重（從總獎勵中扣除）
    stability_bonus: 0.1    # 穩定獎勵（當誤差和速度都很小時給予額外獎勵）
    stable_error_thresh: 0.1 # 視為穩定的誤差閾值
    stable_vel_thresh: 0.2   # 視為穩定的速度閾值

  # 連續型獎勵權重（reward_type: "continuous" 時使用）
  lambda_error: 1.0         # Tracking error weight
  lambda_velocity: 0.2       # Velocity error weight
  lambda_control: 0.01       # Control effort weight
  lambda_overshoot: 0.05     # Overshoot penalty weight
  
  # 任務完成型獎勵參數（reward_type: "task_completion" 時使用）
  task_completion:
    stable_threshold: 0.1      # 穩定閾值
    stable_timesteps: 50       # 需要穩定的時間步數
    effective_speed_multiplier: 5.0  # 有效速度乘數
    distance_scale: 1.0         # 距離縮放

# Episode Configuration
episode:
  max_steps: 1000           # Maximum RL steps per episode (50 seconds at 20 Hz)
  termination_threshold: 5.0  # Terminate if |x| or |ẋ| exceeds this value (stability check)

# PPO Training Configuration
training:
  # Policy Network Architecture
  policy_net_arch: [128, 128]    # 2 layers, 128 units each (recommended: 64-128 for 9D state)
  value_net_arch: [128, 128]     # Value network architecture

  # PPO Hyperparameters (Recommended Settings)
  total_timesteps: 5000000 #  100000
  learning_rate: 0.0003          # 3 × 10⁻⁴ (standard starting value)
  n_steps: 2048                  # Trajectory collection length
  batch_size: 64                 # Mini-batch size (should be < n_steps)
  gamma: 0.99                    # Discount factor (high for long-term planning)
  gae_lambda: 0.95               # GAE lambda (balances bias/variance in advantage estimation)
  clip_range: 0.2                # PPO clipping parameter
  ent_coef: 0.0                  # Entropy coefficient (exploration)
  vf_coef: 0.5                   # Value function coefficient
  max_grad_norm: 0.5             # Gradient clipping
  n_epochs: 10                   # Number of epochs per update

  # Normalization (CRITICAL for stability)
  use_vec_normalize: true        # Use VecNormalize wrapper for obs/reward normalization

  # Quick Test Mode (參考 AirPilot 快速驗證模式)
  quick_test_mode: false        # 設為 false 使用完整訓練模式（建議至少 100,000 步）
  quick_test_timesteps: 20000    # 快速模式訓練步數（AirPilot 風格）
  quick_test_net_arch: [64, 64]  # 快速模式網路架構（較小，AirPilot 風格）

# Logging
logging:
  tensorboard_log: "./ppo_pid_logs/"
  save_path: "./models/"
  checkpoint_freq: 100000
