# DPPO PID Controller Configuration
# Based on Research Plan Phase 1: Single-Axis Environment Setup

# Plant Dynamics (Second-Order System - Angular Rate Control)
plant:
  J: 1.0                    # Inertia (moment of inertia)
  B: 0.5                    # Damping coefficient
  u_min: -10.0              # Minimum control input (actuator limit)
  u_max: 10.0               # Maximum control input (actuator limit)
  integration_method: "rk4" # Integration method: "euler" or "rk4" (RK4 recommended)

# Time Step Configuration
timing:
  dt_inner: 0.005           # Inner loop time step (200 Hz)
  dt_outer: 0.05            # Outer loop time step (20 Hz)
  n_inner_steps: 10         # Steps per RL action (dt_outer / dt_inner)

# Reference Signal
reference:
  r_min: -2.0               # Minimum setpoint
  r_max: 2.0                # Maximum setpoint
  change_interval: 2.0      # Seconds between setpoint changes

# Disturbance (Intermittent Pulse Disturbance)
disturbance:
  enabled: true
  magnitude: 0.5            # Max disturbance force: ±0.5
  duration: 0.1             # Duration of disturbance (seconds)
  interval: 5.0             # Average time between disturbances (seconds)
  probability: 0.01         # Probability of disturbance per inner step

# PID Constraints (Phase 1 Specifications)
pid:
  # Initial values (manual baseline)
  kp_init: 5.0              # Initial proportional gain
  ki_init: 0.1              # Initial integral gain
  kd_init: 0.2              # Initial derivative gain

  # Maximum bounds (action clipping)
  kp_max: 10.0              # Maximum proportional gain
  ki_max: 5.0               # Maximum integral gain
  kd_max: 5.0               # Maximum derivative gain

  # Anti-windup
  integral_max: 100.0       # Anti-windup limit for integral term

# Observation Space Normalization
observation:
  error_scale: 2.0
  error_dot_scale: 5.0
  integral_scale: 10.0
  position_scale: 3.0
  velocity_scale: 5.0
  reference_scale: 2.0
  gain_scale: 10.0

# Reward Function Weights
reward:
  lambda_error: 5.0         # Tracking error weight
  lambda_velocity: 0.5      # Velocity error weight
  lambda_control: 0.01      # Control effort weight
  lambda_overshoot: 0.2     # Overshoot penalty weight

# Episode Configuration
episode:
  max_steps: 1000           # Maximum RL steps per episode (50 seconds at 20 Hz)
  termination_threshold: 5.0  # Terminate if |x| or |ẋ| exceeds this value (stability check)

# PPO Training Configuration
training:
  # Policy Network Architecture
  policy_net_arch: [128, 128]    # 2 layers, 128 units each (recommended: 64-128 for 9D state)
  value_net_arch: [128, 128]     # Value network architecture

  # PPO Hyperparameters (Recommended Settings)
  total_timesteps: 5000000
  learning_rate: 0.0003          # 3 × 10⁻⁴ (standard starting value)
  n_steps: 2048                  # Trajectory collection length
  batch_size: 64                 # Mini-batch size (should be < n_steps)
  gamma: 0.99                    # Discount factor (high for long-term planning)
  gae_lambda: 0.95               # GAE lambda (balances bias/variance in advantage estimation)
  clip_range: 0.2                # PPO clipping parameter
  ent_coef: 0.0                  # Entropy coefficient (exploration)
  vf_coef: 0.5                   # Value function coefficient
  max_grad_norm: 0.5             # Gradient clipping
  n_epochs: 10                   # Number of epochs per update

  # Normalization (CRITICAL for stability)
  use_vec_normalize: true        # Use VecNormalize wrapper for obs/reward normalization

# Logging
logging:
  tensorboard_log: "./ppo_pid_logs/"
  save_path: "./models/"
  checkpoint_freq: 100000
