# DPPO PID Controller Configuration
# Based on Research Plan Phase 1: Single-Axis Environment Setup

# Plant Dynamics (Second-Order System - Angular Rate Control)
plant:
  J: 1.0                    # Inertia (moment of inertia)
  B: 0.5                    # Damping coefficient
  u_min: -10.0              # Minimum control input (actuator limit)
  u_max: 10.0               # Maximum control input (actuator limit)
  integration_method: "rk4" # Integration method: "euler" or "rk4" (RK4 recommended)

# Time Step Configuration
timing:
  dt_inner: 0.005           # Inner loop time step (200 Hz)
  dt_outer: 0.05            # Outer loop time step (20 Hz)
  n_inner_steps: 10         # Steps per RL action (dt_outer / dt_inner)

# Reference Signal
reference:
  r_min: -2.0               # Minimum setpoint
  r_max: 2.0                # Maximum setpoint
  change_interval: 2.0      # Seconds between setpoint changes

# Disturbance (Intermittent Pulse Disturbance)
disturbance:
  enabled: true
  magnitude: 0.5            # Max disturbance force: ±0.5
  duration: 0.1             # Duration of disturbance (seconds)
  interval: 5.0             # Average time between disturbances (seconds)
  probability: 0.01         # Probability of disturbance per inner step

# PID Constraints (Phase 1 Specifications)
pid:
  # Controller type (實驗性功能)
  controller_type: "linear"  # "linear" 或 "nonlinear" (AirPilot 風格)
  nonlinear_max_velocity: 1.0  # 非線性 PID 的最大速度限制

  # Initial values (manual baseline)
  kp_init: 5.0              # Initial proportional gain
  ki_init: 0.1              # Initial integral gain
  kd_init: 0.2              # Initial derivative gain

  # Maximum bounds (action clipping)
  kp_max: 10.0              # Maximum proportional gain
  ki_max: 5.0               # Maximum integral gain
  kd_max: 5.0               # Maximum derivative gain

  # Anti-windup
  integral_max: 100.0       # Anti-windup limit for integral term

# Observation Space Normalization
observation:
  error_scale: 2.0
  error_dot_scale: 5.0
  integral_scale: 10.0
  position_scale: 3.0
  velocity_scale: 5.0
  reference_scale: 2.0
  gain_scale: 10.0

# Reward Function Configuration
# 獎勵函數類型選擇：
#   - "continuous": 連續型懲罰獎勵（負數，適合連續跟蹤任務）- 當前默認
#   - "task_completion": 任務完成型獎勵（正數，AirPilot 風格，適合點對點導航）- 實驗性
reward:
  reward_type: "continuous"  # "continuous" 或 "task_completion"（推薦使用 continuous 用於連續跟蹤任務）
  
  # 連續型獎勵權重（reward_type: "continuous" 時使用）
  lambda_error: 1.0         # Tracking error weight (進一步降低，目標獎勵範圍)
  lambda_velocity: 0.2       # Velocity error weight (進一步降低)
  lambda_control: 0.01       # Control effort weight (保持不變)
  lambda_overshoot: 0.05     # Overshoot penalty weight (進一步降低)
  
  # 任務完成型獎勵參數（reward_type: "task_completion" 時使用，AirPilot 風格）
  # ⚠️ 注意：task_completion 獎勵適合點對點導航任務，不適合連續跟蹤任務
  # 如果必須使用，建議調整參數：
  #   - 降低 effective_speed_multiplier（從 10.0 降到 2.0-5.0）以減少極端獎勵值
  #   - 放寬 stable_threshold（從 0.1 提高到 0.2-0.5）以更容易達到穩定
  #   - 減少 stable_timesteps（從 50 降到 20-30）以適應快速變化的設定值
  task_completion:
    stable_threshold: 0.1      # 穩定閾值（誤差小於此值視為穩定）
    stable_timesteps: 50       # 需要穩定的時間步數（AirPilot: 50）
    effective_speed_multiplier: 10.0  # 有效速度乘數（AirPilot: 10，建議降低到 2.0-5.0）
    distance_scale: 1.0         # 距離縮放

# Episode Configuration
episode:
  max_steps: 1000           # Maximum RL steps per episode (50 seconds at 20 Hz)
  termination_threshold: 5.0  # Terminate if |x| or |ẋ| exceeds this value (stability check)

# PPO Training Configuration
training:
  # Policy Network Architecture
  policy_net_arch: [128, 128]    # 2 layers, 128 units each (recommended: 64-128 for 9D state)
  value_net_arch: [128, 128]     # Value network architecture

  # PPO Hyperparameters (Recommended Settings)
  total_timesteps: 100000 #  5000000
  learning_rate: 0.0003          # 3 × 10⁻⁴ (standard starting value)
  n_steps: 2048                  # Trajectory collection length
  batch_size: 64                 # Mini-batch size (should be < n_steps)
  gamma: 0.99                    # Discount factor (high for long-term planning)
  gae_lambda: 0.95               # GAE lambda (balances bias/variance in advantage estimation)
  clip_range: 0.2                # PPO clipping parameter
  ent_coef: 0.0                  # Entropy coefficient (exploration)
  vf_coef: 0.5                   # Value function coefficient
  max_grad_norm: 0.5             # Gradient clipping
  n_epochs: 10                   # Number of epochs per update

  # Normalization (CRITICAL for stability)
  use_vec_normalize: true        # Use VecNormalize wrapper for obs/reward normalization

  # Quick Test Mode (參考 AirPilot 快速驗證模式)
  quick_test_mode: false        # 設為 false 使用完整訓練模式（建議至少 100,000 步）
  quick_test_timesteps: 20000    # 快速模式訓練步數（AirPilot 風格）
  quick_test_net_arch: [64, 64]  # 快速模式網路架構（較小，AirPilot 風格）

# Logging
logging:
  tensorboard_log: "./ppo_pid_logs/"
  save_path: "./models/"
  checkpoint_freq: 100000
