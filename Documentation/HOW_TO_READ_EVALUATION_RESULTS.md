# 如何閱讀評估結果 / How to Read Evaluation Results

## 📊 輸出結構解析 / Output Structure Breakdown

### 1. 標題區塊 / Header Section

```
============================================================
DPPO PID Controller - Evaluation Script
============================================================
Model: .\models\dppo_pid_checkpoint_5000000_steps.zip
Episodes: 5
============================================================
```

**解讀**:
- **Model**: 正在評估的模型檔案路徑
  - `checkpoint_5000000_steps` = 訓練了 500 萬步的檢查點
- **Episodes**: 將執行 5 個評估 episode
  - 每個 episode = 1000 步（根據配置）

---

### 2. 環境設置階段 / Environment Setup Phase

```
[1/3] Creating evaluation environment...
[2/3] No normalization statistics found, evaluating without normalization
[3/3] Loading trained model...
```

**解讀**:
- **[1/3]**: 創建評估環境（與訓練環境相同配置）
- **[2/3]**: 未找到正規化統計資料
  - ⚠️ **注意**: 如果訓練時使用了 `VecNormalize`，應該有 `_vec_normalize.pkl` 檔案
  - 沒有正規化統計 = 評估時觀察值未正規化（可能影響性能）
- **[3/3]**: 載入訓練好的模型權重

**建議**: 如果訓練時使用了正規化，確保評估時也載入相同的正規化統計。

---

### 3. Episode 結果 / Episode Results

#### 單個 Episode 範例

```
Episode 1/5:
  Reward: -30415.88
  Length: 1000 steps
  Final Error: nan
  Mean Abs Error: nan
  Final Gains: Kp=0.000, Ki=0.000, Kd=2.382
------------------------------------------------------------
```

#### 逐項解讀 / Item-by-Item Explanation

##### 3.1 Reward（獎勵值）

**數值**: `-30415.88`

**含義**:
- 這是整個 episode 的**累積總獎勵**
- **負數是正常的**（獎勵函數設計為懲罰誤差）

**如何計算**:
```
每步獎勵 = -λ₁·e² - λ₂·ẋ² - λ₃·u² - λ₄·overshoot_penalty

其中：
- λ₁ = 5.0 (誤差權重)
- λ₂ = 0.5 (速度權重)
- λ₃ = 0.01 (控制努力權重)
- λ₄ = 0.2 (超調權重)

1000 步累積 = 每步平均獎勵 × 1000
```

**解讀指南**:
- **較高的負數**（如 -65,100）= 較差的控制性能
- **較低的負數**（如 -23,225）= 較好的控制性能
- **目標**: 盡可能接近 0（但通常不會達到）

**範例計算**:
```
假設平均誤差 = 2.0
每步誤差懲罰 ≈ -5.0 × (2.0)² = -20.0
1000 步累積 ≈ -20,000

實際觀察: -30,415
→ 平均誤差可能約為 2.5
```

##### 3.2 Length（Episode 長度）

**數值**: `1000 steps`

**含義**:
- Episode 執行的步數
- **1000 步 = 完整 episode**（根據配置 `max_steps: 1000`）

**解讀**:
- ✅ **1000 步**: Episode 完整執行，沒有提前終止
  - 表示系統**穩定**，沒有超出安全邊界
- ⚠️ **< 1000 步**: 提前終止
  - 可能原因：
    - 位置超出安全範圍（`|x| > 5.0`）
    - 系統不穩定

**時間換算**:
```
1000 步 × 0.05 秒/步 = 50 秒（模擬時間）
```

##### 3.3 Final Error（最終誤差）

**數值**: `nan` ⚠️

**含義**:
- Episode 結束時的最終追蹤誤差
- `nan` = 無法計算（歷史記錄為空）

**正常情況應該是**:
```
Final Error: 0.0234  (例如)
```
- 數值越小越好（接近 0 = 完美追蹤）
- 正數或負數都可以（表示在參考值上方或下方）

**為什麼是 nan**:
- 歷史記錄未正確獲取（環境包裝問題）
- 這是需要修復的技術問題

##### 3.4 Mean Abs Error（平均絕對誤差）

**數值**: `nan` ⚠️

**含義**:
- 整個 episode 的平均絕對誤差
- 衡量整體追蹤性能

**正常情況應該是**:
```
Mean Abs Error: 0.1523  (例如)
```
- 數值越小 = 追蹤越準確
- 這是比 Final Error 更重要的指標（考慮整個 episode）

**計算方式**:
```
MAE = (1/N) × Σ|error(t)|
其中 N = episode 步數
```

##### 3.5 Final Gains（最終 PID 增益）

**數值**: `Kp=0.000, Ki=0.000, Kd=2.382`

**含義**:
- Episode 結束時模型選擇的 PID 參數
- 這些是**學習到的**增益值（不是手動設定）

**各增益的作用**:

1. **Kp (比例增益)**:
   - `0.000` = 沒有比例控制
   - 正常範圍: 0.5 - 10.0
   - **作用**: 根據當前誤差產生控制信號
   - **過高**: 系統振盪
   - **過低**: 響應遲緩

2. **Ki (積分增益)**:
   - `0.000` = 沒有積分控制
   - 正常範圍: 0.0 - 5.0
   - **作用**: 消除穩態誤差
   - **過高**: 積分飽和、超調
   - **為 0**: 可能表示系統不需要積分項，或訓練中學習到避免積分項

3. **Kd (微分增益)**:
   - `2.382` = 中等強度的微分控制
   - 正常範圍: 0.0 - 5.0
   - **作用**: 預測未來誤差，抑制振盪
   - **過高**: 對噪聲敏感
   - **過低**: 無法抑制振盪

**解讀範例**:
```
Kp=0.000, Ki=0.000, Kd=2.382
→ 純微分控制器（不常見）
→ 可能表示：
   - 系統主要需要阻尼（抑制振盪）
   - 比例和積分項導致不穩定
   - 訓練過程中的探索策略
```

---

### 4. 評估摘要 / Evaluation Summary

```
============================================================
EVALUATION SUMMARY
============================================================
Mean Episode Reward: -40670.03 ± 14524.75
Mean Episode Length: 1000.0 ± 0.0
============================================================
```

#### 4.1 Mean Episode Reward（平均獎勵）

**數值**: `-40670.03 ± 14524.75`

**解讀**:
- **平均值**: -40,670.03
  - 5 個 episode 的平均累積獎勵
- **標準差**: ±14,524.75
  - 變異性指標
  - **較大** = 性能不穩定（不同 episode 表現差異大）
  - **較小** = 性能穩定（表現一致）

**統計意義**:
```
範圍: [-40,670 - 14,525, -40,670 + 14,525]
    = [-55,195, -26,145]

這表示：
- 最差 episode: 約 -55,195
- 最好 episode: 約 -26,145
- 變異係數: 14,525 / 40,670 ≈ 36% (較高變異性)
```

**性能評估**:
- ✅ **變異性 < 20%**: 性能穩定
- ⚠️ **變異性 20-40%**: 中等變異性（當前情況）
- ❌ **變異性 > 40%**: 性能不穩定，需要改進

#### 4.2 Mean Episode Length（平均 Episode 長度）

**數值**: `1000.0 ± 0.0`

**解讀**:
- **平均值**: 1000.0 步
- **標準差**: 0.0
  - 所有 episode 都是完整執行
  - ✅ **好現象**: 系統穩定，沒有提前終止

---

### 5. 視覺化輸出 / Visualization Output

```
Generating visualizations...
  Saved: ./evaluation_results/best_episode_5.png
  Saved: ./evaluation_results/worst_episode_4.png
  Saved: ./evaluation_results/evaluation_summary.png
✓ Visualizations saved to: ./evaluation_results/
```

**解讀**:
- **best_episode_5.png**: 最佳 episode（獎勵最高 = -23,225.69）
- **worst_episode_4.png**: 最差 episode（獎勵最低 = -65,100.37）
- **evaluation_summary.png**: 統計摘要圖表

**圖表內容**:
1. **位置追蹤圖**: 系統位置 vs 參考信號
2. **誤差圖**: 追蹤誤差隨時間變化
3. **控制輸入圖**: PID 輸出信號
4. **PID 增益演化圖**: Kp, Ki, Kd 隨時間變化

---

### 6. 警告訊息 / Warning Messages

```
RuntimeWarning: Mean of empty slice.
RuntimeWarning: invalid value encountered in scalar divide
```

**含義**:
- 嘗試計算空數組的平均值
- 原因: 歷史記錄為空（技術問題）

**影響**:
- 不影響評估執行
- 但無法顯示誤差統計和部分圖表數據

**解決方案**:
- 已在前面的修復中處理
- 需要確保環境包裝正確解包

---

## 📈 整體性能評估 / Overall Performance Assessment

### 當前結果分析

#### ✅ 正面指標

1. **系統穩定性**: 所有 episode 完整執行（1000 步）
2. **無提前終止**: 沒有超出安全邊界
3. **模型載入成功**: 模型正常運行

#### ⚠️ 需要關注的問題

1. **獎勵值較大**: 平均 -40,670（表示誤差較大）
2. **高變異性**: 標準差 ±14,525（36% 變異係數）
3. **Ki 多數為 0**: 可能表示積分項導致不穩定
4. **歷史記錄問題**: 無法計算誤差統計

### 性能等級評估

根據當前結果：

| 指標 | 當前值 | 目標值 | 狀態 |
|------|--------|--------|------|
| 平均獎勵 | -40,670 | > -20,000 | ⚠️ 需改進 |
| 標準差 | ±14,525 | < ±5,000 | ⚠️ 需改進 |
| Episode 長度 | 1000 | 1000 | ✅ 良好 |
| 系統穩定性 | 100% | 100% | ✅ 良好 |

**總體評估**: ⚠️ **中等性能，需要進一步優化**

---

## 🔍 如何解讀 PID 增益模式 / How to Interpret PID Gain Patterns

### 常見模式分析

#### 模式 1: 高 Kd，低 Kp/Ki
```
Kp=0.000, Ki=0.000, Kd=2.382
```
**解讀**:
- 系統主要需要**阻尼**（抑制振盪）
- 可能原因: 系統本身有較大慣性，需要預測性控制

#### 模式 2: 中等 Kp，零 Ki，高 Kd
```
Kp=1.753, Ki=0.000, Kd=4.966
```
**解讀**:
- **PD 控制器**（無積分項）
- 可能原因: 積分項導致不穩定或超調

#### 模式 3: 平衡的增益
```
Kp=2.013, Ki=0.526, Kd=3.048
```
**解讀**:
- **完整的 PID 控制器**
- 這是最理想的配置
- 所有三個項都起作用

---

## 📊 與基準比較 / Comparison with Baseline

### 手動調參 PID（初始值）

根據 `config.yaml`:
```yaml
kp_init: 5.0
ki_init: 0.1
kd_init: 0.2
```

**比較**:
- **手動 PID**: Kp=5.0, Ki=0.1, Kd=0.2
- **RL-PID**: Kp=0-2.3, Ki=0-0.5, Kd=2.4-5.0

**觀察**:
- RL 學習到**更低的 Kp**（避免過度響應）
- RL 學習到**更高的 Kd**（需要更多阻尼）
- RL 學習到**Ki 可為 0**（避免積分飽和）

---

## 🎯 改進建議 / Improvement Recommendations

### 1. 降低獎勵值（提高性能）

**方法**:
- 調整獎勵權重（降低 `lambda_error`）
- 增加訓練步數
- 調整學習率

### 2. 降低變異性（提高穩定性）

**方法**:
- 增加訓練步數
- 使用更穩定的探索策略
- 調整 PPO 超參數（如 `clip_range`）

### 3. 修復歷史記錄問題

**方法**:
- 確保環境正確解包
- 驗證歷史記錄在每個步驟都被記錄

---

## 📝 快速參考表 / Quick Reference Table

| 術語 | 含義 | 好/壞 |
|------|------|-------|
| Reward | 累積獎勵 | 越接近 0 越好 |
| Length | Episode 長度 | 1000 = 完整執行 ✅ |
| Final Error | 最終誤差 | 越小越好（接近 0） |
| Mean Abs Error | 平均誤差 | 越小越好 |
| Kp | 比例增益 | 0.5-10.0 正常範圍 |
| Ki | 積分增益 | 0.0-5.0 正常範圍 |
| Kd | 微分增益 | 0.0-5.0 正常範圍 |
| 標準差 | 變異性 | 越小越好（< 20%） |

---

## 🔗 相關文檔 / Related Documentation

- `EVALUATION_ANALYSIS.md` - 詳細技術分析
- `PPO_HYPERPARAMETERS.md` - 超參數指南
- `README.md` - 項目總覽

---

**最後更新**: 2025-01-XX
**版本**: 1.0

