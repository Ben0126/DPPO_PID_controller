# AirPilot ç¥ç¶“ç¶²è·¯æ¶æ§‹èˆ‡çå‹µå‡½æ•¸è©³ç´°å°æ¯”èˆ‡å¯¦ä½œæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æª”åŸºæ–¼ AirPilot è«–æ–‡çš„è©³ç´°æ¶æ§‹è³‡è¨Šï¼Œæä¾›èˆ‡ç•¶å‰ DPPO_PID_controller å°ˆæ¡ˆçš„å°æ¯”åˆ†æï¼Œä¸¦çµ¦å‡ºå…·é«”çš„å¯¦ä½œå»ºè­°ã€‚

---

## ä¸€ã€ç¥ç¶“ç¶²è·¯æ¶æ§‹å°æ¯”

### 1.1 æ¶æ§‹è¦æ ¼å°æ¯”

| é …ç›® | DPPO_PID_controller (ç•¶å‰) | AirPilot (è«–æ–‡) | å»ºè­° |
|------|---------------------------|----------------|------|
| **å…±äº«å±¤æ¶æ§‹** | [128, 128] | [64, 64] | âœ… å¯é¸ï¼šæ·»åŠ  [64, 64] å¿«é€Ÿæ¨¡å¼ |
| **Actor Head** | 3-dim (Kp, Ki, Kd) | 9-dim (Kp, Ki, Kd Ã— 3è»¸) | âœ… ç•¶å‰è¨­è¨ˆæ­£ç¢ºï¼ˆå–®è»¸ï¼‰ |
| **Critic Head** | 1-dim (V(s)) | 1-dim (V(s)) | âœ… ä¸€è‡´ |
| **æ¿€æ´»å‡½æ•¸** | ReLU (æ¨æ¸¬) | ReLU (æ¨æ¸¬) | âœ… ä¸€è‡´ |
| **åƒæ•¸æ•¸é‡** | ~15K | ~10K | âœ… ç•¶å‰ç¨å¤§ï¼Œä½†å¯æ¥å— |
| **å…±äº«åƒæ•¸** | âœ… æ˜¯ (SB3 é è¨­) | âœ… æ˜¯ | âœ… ä¸€è‡´ |

### 1.2 Stable-Baselines3 çš„å…±äº«æ¶æ§‹

**é‡è¦ç™¼ç¾**ï¼šSB3 çš„ `MlpPolicy` **é è¨­å°±æ˜¯å…±äº«åƒæ•¸æ¶æ§‹**ï¼

```python
# SB3 å…§éƒ¨å¯¦ç¾ï¼ˆç°¡åŒ–ç‰ˆï¼‰
class ActorCriticPolicy:
    def __init__(self, ...):
        # å…±äº«ç‰¹å¾µæå–å±¤
        self.features_extractor = ...
        
        # åˆ†é›¢çš„è¼¸å‡ºé ­
        self.action_net = ...  # Actor head
        self.value_net = ...   # Critic head
```

**ç•¶å‰é…ç½®å·²ç¶“æ­£ç¢º**ï¼š
```python
# train.py ä¸­çš„é…ç½®
policy_kwargs=dict(
    net_arch=[dict(pi=policy_net, vf=value_net)]
)
# SB3 æœƒè‡ªå‹•å…±äº«å‰å¹¾å±¤ï¼ˆå¦‚æœ pi å’Œ vf ç›¸åŒï¼‰
```

### 1.3 å…·é«”é…ç½®å»ºè­°

#### é¸é … 1: ä¿æŒç•¶å‰æ¶æ§‹ï¼ˆæ¨è–¦ï¼‰

```yaml
# config.yaml
training:
  policy_net_arch: [128, 128]  # ç•¶å‰è¨­å®š
  value_net_arch: [128, 128]   # ç•¶å‰è¨­å®š
```

**å„ªé»**ï¼š
- æ›´å¤§çš„å®¹é‡ï¼Œå¯èƒ½å­¸ç¿’æ›´è¤‡é›œçš„ç­–ç•¥
- å·²ç¶“é©—è­‰å¯å·¥ä½œ

#### é¸é … 2: æ·»åŠ  AirPilot é¢¨æ ¼çš„å¿«é€Ÿæ¨¡å¼

```yaml
# config.yaml
training:
  # æ¨™æº–æ¨¡å¼ï¼ˆç•¶å‰ï¼‰
  policy_net_arch: [128, 128]
  value_net_arch: [128, 128]
  
  # å¿«é€Ÿé©—è­‰æ¨¡å¼ï¼ˆåƒè€ƒ AirPilotï¼‰
  quick_test_mode: false
  quick_test_net_arch: [64, 64]  # AirPilot çš„æ¶æ§‹
  quick_test_timesteps: 20000
```

**å¯¦ä½œ**ï¼ˆåœ¨ `train.py` ä¸­ï¼‰ï¼š
```python
# æª¢æŸ¥å¿«é€Ÿæ¨¡å¼
if config['training'].get('quick_test_mode', False):
    net_arch = config['training']['quick_test_net_arch']
    total_timesteps = config['training']['quick_test_timesteps']
    print("âš ï¸ å¿«é€Ÿè¨“ç·´æ¨¡å¼ï¼ˆAirPilot é¢¨æ ¼ï¼‰")
else:
    net_arch = config['training']['policy_net_arch']
    total_timesteps = config['training']['total_timesteps']

# å‰µå»ºæ¨¡å‹ï¼ˆSB3 è‡ªå‹•å…±äº«åƒæ•¸ï¼‰
model = PPO(
    policy="MlpPolicy",
    env=train_env,
    policy_kwargs=dict(
        net_arch=[dict(pi=net_arch, vf=net_arch)]  # ç›¸åŒ = å…±äº«
    ),
    # ... å…¶ä»–åƒæ•¸
)
```

---

## äºŒã€çå‹µå‡½æ•¸è¨­è¨ˆå°æ¯”

### 2.1 æ ¸å¿ƒå·®ç•°

| ç‰¹æ€§ | DPPO_PID_controller | AirPilot | é©ç”¨å ´æ™¯ |
|------|---------------------|----------|---------|
| **é¡å‹** | é€£çºŒå‹ï¼ˆæ¯æ­¥éƒ½æœ‰çå‹µï¼‰ | ä»»å‹™å®Œæˆå‹ï¼ˆåˆ°é”ç›®æ¨™å¾Œçµ¦å¤§çå‹µï¼‰ | ä¸åŒ |
| **ä¸»è¦çå‹µ** | `-Î»â‚eÂ² - Î»â‚‚áº‹Â² - Î»â‚ƒuÂ² - Î»â‚„max(0,eÂ·Ä—)` | `e^(EffectiveSpeed Ã— 10)` | ä¸åŒ |
| **ç©©å®šæ€§è¦æ±‚** | éš±å¼ï¼ˆé€šéæ‡²ç½°æŒ¯ç›ªï¼‰ | é¡¯å¼ï¼ˆ50 timesteps ç©©å®šï¼‰ | ä¸åŒ |
| **é©ç”¨ä»»å‹™** | é€£çºŒè·Ÿè¹¤ä»»å‹™ | é»å°é»å°èˆªä»»å‹™ | ä¸åŒ |

### 2.2 ç•¶å‰çå‹µå‡½æ•¸åˆ†æ

```python
# dppo_pid_env.py ä¸­çš„ç•¶å‰å¯¦ç¾
reward = -Î»_error * errorÂ²           # è¿½è¹¤èª¤å·®
        - Î»_velocity * velocityÂ²     # é€Ÿåº¦æ‡²ç½°
        - Î»_control * uÂ²             # æ§åˆ¶åŠªåŠ›
        - Î»_overshoot * max(0, eÂ·Ä—)  # è¶…èª¿æ‡²ç½°
```

**å„ªé»**ï¼š
- âœ… æ¯æ­¥éƒ½æœ‰ä¿¡è™Ÿï¼Œé©åˆé€£çºŒæ§åˆ¶
- âœ… å¤šç›®æ¨™å¹³è¡¡ï¼ˆç²¾åº¦ã€ç©©å®šæ€§ã€æ•ˆç‡ï¼‰
- âœ… é©åˆç•¶å‰ä»»å‹™ï¼ˆé€£çºŒè·Ÿè¹¤ï¼‰

**ç¼ºé»**ï¼š
- âš ï¸ å¯èƒ½ä¸å¤ æ¿€å‹µå¿«é€Ÿå®Œæˆä»»å‹™
- âš ï¸ æ²’æœ‰æ˜ç¢ºçš„ã€Œä»»å‹™å®Œæˆã€æ¦‚å¿µ

### 2.3 AirPilot çå‹µå‡½æ•¸åˆ†æ

```python
# AirPilot çš„çå‹µé‚è¼¯ï¼ˆç°¡åŒ–ç‰ˆï¼‰
if stable_counter >= 50:  # ç©©å®š 50 timesteps
    effective_speed = distance / (0.04 * (timestep - 50))
    reward = np.exp(effective_speed * 10)  # æŒ‡æ•¸çå‹µ
    # é‡ç½®ä»»å‹™
else:
    reward = -np.linalg.norm(position_error)  # æ¥è¿‘çå‹µ
```

**å„ªé»**ï¼š
- âœ… å¼·çƒˆæ¿€å‹µå¿«é€Ÿå®Œæˆï¼ˆæŒ‡æ•¸æ”¾å¤§ï¼‰
- âœ… æ˜ç¢ºçš„ç©©å®šæ€§è¦æ±‚
- âœ… é©åˆé»å°é»å°èˆª

**ç¼ºé»**ï¼š
- âš ï¸ ä¸é©åˆé€£çºŒè·Ÿè¹¤ä»»å‹™
- âš ï¸ è¨“ç·´åˆæœŸå¯èƒ½æ²’æœ‰çå‹µä¿¡è™Ÿï¼ˆç„¡æ³•é”åˆ°ç©©å®šï¼‰

### 2.4 æ··åˆçå‹µå‡½æ•¸è¨­è¨ˆï¼ˆå»ºè­°ï¼‰

**ç›®æ¨™**ï¼šçµåˆå…©è€…å„ªé»ï¼Œæ”¯æŒå…©ç¨®ä»»å‹™æ¨¡å¼

#### å¯¦ä½œæ–¹æ¡ˆï¼šåœ¨ `config.yaml` æ·»åŠ çå‹µé¡å‹é¸é …

```yaml
# config.yaml
reward:
  # çå‹µå‡½æ•¸é¡å‹
  reward_type: "continuous"  # "continuous" æˆ– "task_completion"
  
  # é€£çºŒå‹çå‹µï¼ˆç•¶å‰å¯¦ç¾ï¼‰
  lambda_error: 5.0
  lambda_velocity: 0.5
  lambda_control: 0.01
  lambda_overshoot: 0.2
  
  # ä»»å‹™å®Œæˆå‹çå‹µï¼ˆAirPilot é¢¨æ ¼ï¼Œå¯é¸ï¼‰
  task_completion:
    stable_threshold: 0.1      # ç©©å®šé–¾å€¼ï¼ˆç±³ï¼‰
    stable_timesteps: 50       # ç©©å®šæ™‚é–“æ­¥æ•¸
    effective_speed_multiplier: 10.0  # æœ‰æ•ˆé€Ÿåº¦ä¹˜æ•¸
    distance_scale: 1.0         # è·é›¢ç¸®æ”¾
```

#### åœ¨ç’°å¢ƒä¸­å¯¦ç¾

```python
# dppo_pid_env.py ä¿®æ”¹
def _calculate_reward(self, error: float, error_dot: float, u: float) -> float:
    """
    è¨ˆç®—çå‹µï¼ˆæ”¯æŒå…©ç¨®æ¨¡å¼ï¼‰
    """
    reward_type = self.config['reward'].get('reward_type', 'continuous')
    
    if reward_type == 'task_completion':
        return self._calculate_task_completion_reward(error)
    else:
        return self._calculate_continuous_reward(error, error_dot, u)

def _calculate_continuous_reward(self, error: float, error_dot: float, u: float) -> float:
    """ç•¶å‰å¯¦ç¾çš„é€£çºŒå‹çå‹µ"""
    error_penalty = -self.lambda_error * error**2
    velocity_penalty = -self.lambda_velocity * self.x_dot**2
    control_penalty = -self.lambda_control * u**2
    overshoot_penalty = -self.lambda_overshoot * max(0, error * error_dot)
    return error_penalty + velocity_penalty + control_penalty + overshoot_penalty

def _calculate_task_completion_reward(self, error: float) -> float:
    """
    ä»»å‹™å®Œæˆå‹çå‹µï¼ˆAirPilot é¢¨æ ¼ï¼‰
    
    æ³¨æ„ï¼šéœ€è¦é¡å¤–çš„ç‹€æ…‹è¿½è¹¤
    """
    # åˆå§‹åŒ–ç‹€æ…‹ï¼ˆåœ¨ __init__ æˆ– reset ä¸­ï¼‰
    if not hasattr(self, 'task_start_pos'):
        self.task_start_pos = self.x
        self.task_target_pos = self.reference
        self.task_distance = abs(self.task_target_pos - self.task_start_pos)
        self.task_stable_counter = 0
        self.task_timestep = 0
    
    self.task_timestep += 1
    abs_error = abs(error)
    
    # æª¢æŸ¥ç©©å®šæ€§
    stable_threshold = self.config['reward']['task_completion']['stable_threshold']
    if abs_error < stable_threshold:
        self.task_stable_counter += 1
    else:
        self.task_stable_counter = 0
    
    # è¨ˆç®—çå‹µ
    stable_timesteps = self.config['reward']['task_completion']['stable_timesteps']
    if self.task_stable_counter >= stable_timesteps:
        # é”åˆ°ç©©å®š - è¨ˆç®—æœ‰æ•ˆé€Ÿåº¦
        time_taken = self.dt_outer * (self.task_timestep - stable_timesteps)
        if time_taken > 0:
            effective_speed = self.task_distance / time_taken
            multiplier = self.config['reward']['task_completion']['effective_speed_multiplier']
            reward = np.exp(effective_speed * multiplier)
            
            # é‡ç½®ä»»å‹™ï¼ˆç”Ÿæˆæ–°ç›®æ¨™ï¼‰
            self.reference = self.np_random.uniform(self.r_min, self.r_max)
            self.task_start_pos = self.x
            self.task_target_pos = self.reference
            self.task_distance = abs(self.task_target_pos - self.task_start_pos)
            self.task_stable_counter = 0
            self.task_timestep = 0
            
            return reward
        else:
            return 0.0
    else:
        # æœªé”ç©©å®š - æ¥è¿‘çå‹µ
        return -abs_error
```

**æ³¨æ„**ï¼šä»»å‹™å®Œæˆå‹çå‹µéœ€è¦ä¿®æ”¹ç’°å¢ƒé‚è¼¯ï¼Œå¯èƒ½å½±éŸ¿ç¾æœ‰åŠŸèƒ½ã€‚å»ºè­°ä½œç‚º**å¯¦é©—æ€§åŠŸèƒ½**ã€‚

---

## ä¸‰ã€è¨“ç·´æŒ‡æ¨™å¯è¦–åŒ–

### 3.1 AirPilot çš„è¨“ç·´æŒ‡æ¨™

è«–æ–‡ä¸­çš„ Fig.14-16 å±•ç¤ºäº†ä¸‰å€‹é—œéµæŒ‡æ¨™ï¼š
- **Effective Speed** vs Training Timesteps
- **Settling Time** vs Training Timesteps
- **Overshoot** vs Training Timesteps

### 3.2 å¯¦ä½œè¨“ç·´æŒ‡æ¨™è¿½è¹¤

#### æ­¥é©Ÿ 1: å‰µå»ºæŒ‡æ¨™è¿½è¹¤å·¥å…·

```python
# utils/training_metrics.py
import numpy as np
from typing import Dict, List
import json
import os

class TrainingMetricsTracker:
    """
    è¿½è¹¤è¨“ç·´éç¨‹ä¸­çš„é—œéµæŒ‡æ¨™ï¼ˆåƒè€ƒ AirPilot Fig.14-16ï¼‰
    """
    
    def __init__(self, log_dir: str = "./training_metrics/"):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        # æŒ‡æ¨™å­˜å„²
        self.timesteps = []
        self.effective_speeds = []
        self.settling_times = []
        self.overshoots = []
        self.mean_errors = []
        
    def log_episode(self, 
                    timestep: int,
                    episode_history: Dict,
                    target_positions: List[float] = None):
        """
        è¨˜éŒ„ä¸€å€‹å›åˆçš„æŒ‡æ¨™
        
        Args:
            timestep: ç•¶å‰è¨“ç·´æ­¥æ•¸
            episode_history: å›åˆæ­·å²ï¼ˆå¾ env.get_history()ï¼‰
            target_positions: ç›®æ¨™ä½ç½®åˆ—è¡¨ï¼ˆç”¨æ–¼è¨ˆç®— settling timeï¼‰
        """
        if not episode_history or not episode_history.get('position'):
            return
        
        positions = np.array(episode_history['position'])
        references = np.array(episode_history['reference'])
        errors = np.array(episode_history['error'])
        times = np.array(episode_history['time'])
        
        # 1. è¨ˆç®—å¹³å‡èª¤å·®
        mean_error = np.mean(np.abs(errors))
        self.mean_errors.append(mean_error)
        
        # 2. è¨ˆç®—æœ‰æ•ˆé€Ÿåº¦ï¼ˆå¦‚æœé©ç”¨ï¼‰
        # æ³¨æ„ï¼šé€™éœ€è¦ä»»å‹™å®Œæˆå‹çå‹µï¼Œå¦å‰‡ç‚º NaN
        effective_speed = self._calculate_effective_speed(
            positions, references, times
        )
        self.effective_speeds.append(effective_speed)
        
        # 3. è¨ˆç®—ç©©å®šæ™‚é–“
        settling_time = self._calculate_settling_time(
            errors, times, threshold=0.02  # 2% èª¤å·®
        )
        self.settling_times.append(settling_time)
        
        # 4. è¨ˆç®—è¶…èª¿
        overshoot = self._calculate_overshoot(
            positions, references
        )
        self.overshoots.append(overshoot)
        
        # 5. è¨˜éŒ„æ™‚é–“æ­¥
        self.timesteps.append(timestep)
    
    def _calculate_effective_speed(self, positions, references, times):
        """
        è¨ˆç®—æœ‰æ•ˆé€Ÿåº¦ï¼ˆAirPilot Eq.9ï¼‰
        
        æ³¨æ„ï¼šé€™éœ€è¦ä»»å‹™å®Œæˆå‹å ´æ™¯
        """
        # ç°¡åŒ–å¯¦ç¾ï¼šè¨ˆç®—å¹³å‡é€Ÿåº¦
        if len(positions) < 2:
            return np.nan
        
        distances = np.diff(positions)
        time_diffs = np.diff(times)
        
        if np.sum(time_diffs) > 0:
            avg_speed = np.sum(np.abs(distances)) / np.sum(time_diffs)
            return avg_speed
        return np.nan
    
    def _calculate_settling_time(self, errors, times, threshold=0.02):
        """
        è¨ˆç®—ç©©å®šæ™‚é–“ï¼ˆé”åˆ° Â±threshold èª¤å·®å…§çš„æ™‚é–“ï¼‰
        """
        abs_errors = np.abs(errors)
        target_error = threshold * np.max(np.abs(errors)) if np.max(np.abs(errors)) > 0 else threshold
        
        # æ‰¾åˆ°æœ€å¾Œä¸€æ¬¡è¶…éé–¾å€¼çš„æ™‚é–“
        above_threshold = abs_errors > target_error
        if np.any(above_threshold):
            last_above_idx = np.where(above_threshold)[0][-1]
            if last_above_idx < len(times) - 1:
                return times[last_above_idx + 1] - times[0]
        
        return times[-1] - times[0]  # æ•´å€‹å›åˆæ™‚é–“
    
    def _calculate_overshoot(self, positions, references):
        """
        è¨ˆç®—è¶…èª¿é‡
        """
        errors = positions - references
        max_overshoot = np.max(np.abs(errors))
        return max_overshoot
    
    def save(self, filename: str = "training_metrics.json"):
        """ä¿å­˜æŒ‡æ¨™åˆ° JSON"""
        data = {
            'timesteps': self.timesteps,
            'effective_speeds': [float(x) if not np.isnan(x) else None for x in self.effective_speeds],
            'settling_times': [float(x) if not np.isnan(x) else None for x in self.settling_times],
            'overshoots': [float(x) if not np.isnan(x) else None for x in self.overshoots],
            'mean_errors': [float(x) for x in self.mean_errors]
        }
        
        filepath = os.path.join(self.log_dir, filename)
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"âœ“ è¨“ç·´æŒ‡æ¨™å·²ä¿å­˜åˆ°: {filepath}")
    
    def load(self, filename: str = "training_metrics.json"):
        """å¾ JSON è¼‰å…¥æŒ‡æ¨™"""
        filepath = os.path.join(self.log_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                data = json.load(f)
            self.timesteps = data['timesteps']
            self.effective_speeds = [x if x is not None else np.nan for x in data['effective_speeds']]
            self.settling_times = [x if x is not None else np.nan for x in data['settling_times']]
            self.overshoots = [x if x is not None else np.nan for x in data['overshoots']]
            self.mean_errors = data['mean_errors']
            return True
        return False
```

#### æ­¥é©Ÿ 2: å‰µå»ºå¯è¦–åŒ–å‡½æ•¸

```python
# utils/visualization.py æ·»åŠ 
import matplotlib.pyplot as plt
import numpy as np
from utils.training_metrics import TrainingMetricsTracker

def plot_airpilot_style_metrics(metrics_tracker: TrainingMetricsTracker, 
                                output_dir: str = "./training_metrics/"):
    """
    ç¹ªè£½ AirPilot é¢¨æ ¼çš„è¨“ç·´æŒ‡æ¨™åœ–è¡¨ï¼ˆFig.14-16ï¼‰
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    timesteps = np.array(metrics_tracker.timesteps)
    
    # Fig.14: Effective Speed vs Timesteps
    effective_speeds = np.array(metrics_tracker.effective_speeds)
    valid_mask = ~np.isnan(effective_speeds)
    
    axes[0].plot(timesteps[valid_mask], effective_speeds[valid_mask], 
                 'b-', linewidth=2, label='Effective Speed')
    axes[0].axhline(y=0.92, color='r', linestyle='--', 
                    label='Fine-tuned PID baseline', linewidth=2)
    axes[0].set_xlabel('Training Timesteps', fontsize=12)
    axes[0].set_ylabel('Effective Speed (m/s)', fontsize=12)
    axes[0].set_title('Effective Speed vs Training Timesteps', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Fig.15: Settling Time vs Timesteps
    settling_times = np.array(metrics_tracker.settling_times)
    valid_mask = ~np.isnan(settling_times)
    
    axes[1].plot(timesteps[valid_mask], settling_times[valid_mask], 
                 'g-', linewidth=2, label='Settling Time')
    axes[1].set_xlabel('Training Timesteps', fontsize=12)
    axes[1].set_ylabel('Settling Time (s)', fontsize=12)
    axes[1].set_title('Settling Time vs Training Timesteps', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # Fig.16: Overshoot vs Timesteps
    overshoots = np.array(metrics_tracker.overshoots)
    valid_mask = ~np.isnan(overshoots)
    
    axes[2].plot(timesteps[valid_mask], overshoots[valid_mask], 
                 'r-', linewidth=2, label='Overshoot')
    axes[2].set_xlabel('Training Timesteps', fontsize=12)
    axes[2].set_ylabel('Overshoot (m)', fontsize=12)
    axes[2].set_title('Overshoot vs Training Timesteps', fontsize=14, fontweight='bold')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜
    filepath = os.path.join(output_dir, 'airpilot_style_metrics.png')
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    print(f"âœ“ è¨“ç·´æŒ‡æ¨™åœ–è¡¨å·²ä¿å­˜åˆ°: {filepath}")
    plt.close()
```

#### æ­¥é©Ÿ 3: åœ¨è¨“ç·´è…³æœ¬ä¸­é›†æˆ

```python
# train.py ä¿®æ”¹
from utils.training_metrics import TrainingMetricsTracker

def train(config_path: str = "config.yaml", ...):
    # ... ç¾æœ‰ä»£ç¢¼ ...
    
    # å‰µå»ºæŒ‡æ¨™è¿½è¹¤å™¨
    metrics_tracker = TrainingMetricsTracker()
    
    # è‡ªå®šç¾©å›èª¿ï¼ˆåœ¨ EvalCallback ä¸­ï¼‰
    class MetricsCallback(EvalCallback):
        def __init__(self, *args, metrics_tracker=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.metrics_tracker = metrics_tracker
        
        def _on_step(self) -> bool:
            # åœ¨è©•ä¼°æ™‚è¨˜éŒ„æŒ‡æ¨™
            if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
                # ç²å–è©•ä¼°ç’°å¢ƒçš„æ­·å²
                # ... å¯¦ç¾ç´°ç¯€ ...
                pass
            return super()._on_step()
    
    # ä½¿ç”¨è‡ªå®šç¾©å›èª¿
    eval_callback = MetricsCallback(
        eval_env,
        metrics_tracker=metrics_tracker,
        # ... å…¶ä»–åƒæ•¸
    )
    
    # è¨“ç·´å¾Œä¿å­˜æŒ‡æ¨™
    model.learn(...)
    metrics_tracker.save()
    
    # ç¹ªè£½åœ–è¡¨
    from utils.visualization import plot_airpilot_style_metrics
    plot_airpilot_style_metrics(metrics_tracker)
```

---

## å››ã€å¯¦æ–½å„ªå…ˆç´šå»ºè­°

### ğŸ”´ é«˜å„ªå…ˆç´šï¼ˆç«‹å³å¯¦æ–½ï¼‰

1. **æ·»åŠ  [64, 64] å¿«é€Ÿè¨“ç·´æ¨¡å¼**
   - æ™‚é–“ï¼š30 åˆ†é˜
   - åƒ¹å€¼ï¼šå¿«é€Ÿé©—è­‰ï¼Œå°æ¯” AirPilot æ€§èƒ½

2. **æ·»åŠ è¨“ç·´æŒ‡æ¨™å¯è¦–åŒ–**
   - æ™‚é–“ï¼š2-3 å°æ™‚
   - åƒ¹å€¼ï¼šæ›´å¥½çš„è¨“ç·´ç›£æ§

### ğŸŸ¡ ä¸­å„ªå…ˆç´šï¼ˆçŸ­æœŸè€ƒæ…®ï¼‰

3. **å¯¦ç¾ä»»å‹™å®Œæˆå‹çå‹µï¼ˆå¯¦é©—æ€§ï¼‰**
   - æ™‚é–“ï¼š3-4 å°æ™‚
   - åƒ¹å€¼ï¼šå°æ¯”ä¸åŒçå‹µå‡½æ•¸çš„æ•ˆæœ
   - **æ³¨æ„**ï¼šéœ€è¦è¬¹æ…æ¸¬è©¦ï¼Œå¯èƒ½å½±éŸ¿ç¾æœ‰åŠŸèƒ½

### ğŸŸ¢ ä½å„ªå…ˆç´šï¼ˆé•·æœŸè€ƒæ…®ï¼‰

4. **å®Œæ•´å¯¦ç¾ AirPilot é¢¨æ ¼çš„ç’°å¢ƒ**
   - åƒ…ç•¶éœ€è¦é»å°é»å°èˆªä»»å‹™æ™‚

---

## äº”ã€é—œéµè¦é»ç¸½çµ

### ç¥ç¶“ç¶²è·¯æ¶æ§‹

âœ… **ç•¶å‰é…ç½®å·²ç¶“æ­£ç¢º**ï¼šSB3 é è¨­å…±äº«åƒæ•¸  
âœ… **å¯é¸å„ªåŒ–**ï¼šæ·»åŠ  [64, 64] å¿«é€Ÿæ¨¡å¼  
âœ… **ç„¡éœ€å¤§å¹…ä¿®æ”¹**ï¼šæ¶æ§‹è¨­è¨ˆå·²ç¶“ç¬¦åˆæœ€ä½³å¯¦è¸

### çå‹µå‡½æ•¸

âœ… **ç•¶å‰è¨­è¨ˆé©åˆé€£çºŒè·Ÿè¹¤ä»»å‹™**  
âš ï¸ **AirPilot è¨­è¨ˆé©åˆé»å°é»å°èˆª**  
ğŸ’¡ **å»ºè­°**ï¼šä¿æŒç•¶å‰è¨­è¨ˆï¼Œä»»å‹™å®Œæˆå‹ä½œç‚ºå¯¦é©—æ€§åŠŸèƒ½

### è¨“ç·´æŒ‡æ¨™

âœ… **å»ºè­°æ·»åŠ **ï¼šEffective Speed, Settling Time, Overshoot è¿½è¹¤  
âœ… **åƒ¹å€¼**ï¼šæ›´å¥½çš„è¨“ç·´ç›£æ§å’Œå°æ¯”åˆ†æ

---

**æ–‡ä»¶ç‰ˆæœ¬**ï¼š1.0  
**å»ºç«‹æ—¥æœŸ**ï¼š2025-01-XX  
**æœ€å¾Œæ›´æ–°**ï¼š2025-01-XX

